load("H:/.shortcut-targets-by-id/1-95-jjFcJ-99DqRccBGFmRS9ZqRftxBC/AnalysisOfEPU/AnalysisOfEPU_Buliskeria/DataScraping/Germany2023/Q2.RData")
load("H:/.shortcut-targets-by-id/1-95-jjFcJ-99DqRccBGFmRS9ZqRftxBC/AnalysisOfEPU/AnalysisOfEPU_Buliskeria/DataScraping/Germany2023/FAZQ2_Zeitung.RData")
library("ggplot2")
library("ggthemes")
library("tidyverse")
library("ggpubr")
library("cowplot")
remove.packages("rlang")
install.packages("rlang")
install.packages("rlang")
library(tidyverse)
install.packages(c("cli", "tibble", "utf8", "vctrs"))
install.packages(c("cli", "tibble", "utf8", "vctrs"))
install.packages(c("cli", "tibble", "utf8", "vctrs"))
install.packages(c("cli", "tibble", "utf8", "vctrs"))
install.packages(c("cli", "tibble", "utf8", "vctrs"))
install.packages(c("cli", "tibble", "utf8", "vctrs"))
library("tidyverse")
devtools::install_github("tidyverse/tidyverse")
install.packages("devtools")
install.packages("rlang")
install.packages("rlang")
library("tidyverse")
library("ggpubr")
remove.packages("ggpubr")
install.packages("ggpubr")
library("ggpubr")
library(ggpubr)
remove.packages("dplyr")
install.packages("dplyr")
library(dplyr)
install.packages("magrittr")
library(dplyr)
detach("package:dplyr", unload = TRUE)
library("ggplot2")
library("ggthemes")
library("scales")
library("tidyverse")
library("tidyverse")
library("tidyverse")
library(tidyverse)
library(tidyverse)
detach("package:tidyverse", unload = TRUE)
library(tidyverse)
library(tidyverse)
detach("package:tidyverse", unload = TRUE)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
update.packages("tidyverse")
installed.packages()
library(tidyverse)
remove.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
packageDescription("tidyverse")$Depends
library(tidyverse)
install.packages("vctrs")
install.packages("vctrs")
library(tidyverse)
install.packages("tidyr")
install.packages("vctrs")
install.packages("vctrs")
library(tidyverse)
remove.packages("vctrs")
install.packages("vctrs")
install.packages("vctrs")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
summary(cars)
summary(cars)
summary(cars)
#set directory
setwd("H:/My Drive/P-Hacking/Identification")
Main  <- read_excel("Dataset.xlsx", sheet = "Main" )
library("tidyverse")
library("ggplot2")
library("scales")
library("readxl")
library("cowplot")
library("patchwork") #putting plots together
library("dplyr")
#set directory
setwd("H:/My Drive/P-Hacking/Identification")
Main  <- read_excel("Dataset.xlsx", sheet = "Main" )
colnames(df)
#set directory
setwd("H:/My Drive/P-Hacking/Identification")
df  <- read_excel("Dataset.xlsx", sheet = "Main" )
colnames(df)
View(Main)
rm(Main)
summary(df)
colnames(df)
summary(df)
colnames(df)
summary(df)
colnames(df)
summary(df)
#set directory
setwd("H:/My Drive/P-Hacking/Identification")
df  <- read_excel("Dataset.xlsx", sheet = "Main" )
colnames(df)
```{r Import Data, include=FALSE}
colnames(df)
summary(df)
colnames(df)
colnames(df)
colnames(df)
R.version.string
update.packages(ask = FALSE, checkBuilt = TRUE)
gitcreds::gitcreds_set()
usethis::git_remotes()
remotes::install_github("rstudio/blogdown")
usethis::create_project()
blogdown::new_site(theme = "wowchemy/starter-academic")
wd()
usethis::create_project("C:/Users/Nino/GitRepo/webpage")
blogdown::new_site(theme = "wowchemy/starter-academic")
index_estimates  <- read_excel("H:\My Drive\~P-Hacking\0.0.STATA\resultsLinear\LinearNeverPublished.xlsx") #209766 obs
library(readxl)
LinearNeverPublished <- read_excel("H:/My Drive/~P-Hacking/0.0.STATA/resultsLinear/LinearNeverPublished.xlsx")
View(LinearNeverPublished)
LinearPublished <- read_excel("H:/My Drive/~P-Hacking/0.0.STATA/resultsLinear/LinearPublished.xlsx")
View(LinearPublished)
LinearPublished <- read_excel("H:/My Drive/~P-Hacking/0.0.STATA/resultsLinear/LinearPublished.xlsx", sheet = 'results2')
View(LinearPublished)
index_estimates <- abs(LinearPublished$FEcoef/LinearPublished$BEcoef)
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
ci_mean
index_estimates <- !is.na(abs(LinearPublished$FEcoef/LinearPublished$BEcoef))
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
ci_mean
mean_of_estimates
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
sample_data
medians
numeric
medians
bootstrap_samples
men(index_estimates)
mean(index_estimates)
median(index_estimates)
index_estimates
index_estimates <-  abs(LinearPublished$FEcoef/LinearPublished$BEcoef)
index_estimates
index_estimates <- index_estimates[!is.na(index_estimates)]
index_estimates
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
# Bootstrap Confidence Interval for the Median
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
ci_median
mean_of_estimates
medians
ci_median
median(index_estimates)
View(LinearPublished)
View(LinearPublished)
LinearPublished <- LinearPublished[LinearPublished$N >20, ]
index_estimates <-  abs(LinearPublished$FEcoef/LinearPublished$BEcoef)
index_estimates <- index_estimates[!is.na(index_estimates)]
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
# Bootstrap Confidence Interval for the Median
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
median(index_estimates)
MAIVEresults <- read_excel("H:/My Drive/~P-Hacking/0.1.MAIVE/MAIVEresults_2N.xlsx")
View(MAIVEresults)
MAIVEresultsFE <- read_excel("H:/My Drive/~P-Hacking/0.1.MAIVE/MAIVEresultsFE_2N.xlsx")
MAIVEresultsBE <- read_excel("H:/My Drive/~P-Hacking/0.1.MAIVE/MAIVEresultsBE_2N.xlsx")
MAIVEresults <- MAIVEresults[MAIVEresults$obs >20, ]
MAIVEresultsFE <- MAIVEresultsFE[MAIVEresultsFE$obs >20, ]
MAIVEresultsBE <- MAIVEresultsBE[MAIVEresultsBE$obs >20, ]
index_estimates <-  abs(MAIVEresultsFE$coef/MAIVEresultsBE$coef)
index_estimates <- index_estimates[!is.na(index_estimates)]
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
# Bootstrap Confidence Interval for the Median
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
index_estimates
index_estimates > 0
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
mean_of_estimates
MAIVEresults <- read_excel("H:/My Drive/~P-Hacking/0.0.STATA/results/results separate/mergemaive_2N.xlsx")
index_estimates <- MAIVEresults$FE_coef[MAIVEresults$FE_obs > 20] / MAIVEresults$BE_coef[MAIVEresults$FE_obs > 20]
index_estimates <- index_estimates[!is.na(index_estimates)]
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
# Bootstrap Confidence Interval for the Median
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
index_estimates
index_estimates <- abs(MAIVEresults$FE_coef[MAIVEresults$FE_obs > 20] / MAIVEresults$BE_coef[MAIVEresults$FE_obs > 20])
index_estimates <- index_estimates[!is.na(index_estimates)]
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
# Bootstrap Confidence Interval for the Median
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
index_estimates
mean(index_estimates)
index_estimates <- index_estimates[!is.infinite(index_estimates)]
index_estimates
index_estimates <- index_estimates[!is.na(index_estimates)]
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
# Bootstrap Confidence Interval for the Median
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
mean(index_estimates)
median(index_estimates)
MAIVEresults <- MAIVEresults[MAIVEresults$FE_obs >20 & MAIVEresults$FE_Ftest>10 & MAIVEresults$BE_Ftest>10, ]
index_estimates <- abs(MAIVEresults$FE_coef  / MAIVEresults$BE_coef)
index_estimates <- index_estimates[!is.infinite(index_estimates)]
index_estimates <- index_estimates[!is.na(index_estimates)]
# Confidence Interval for the Mean
mean_of_estimates <- mean(index_estimates)
sd_of_estimates <- sd(index_estimates)
n <- length(index_estimates)
error_margin <- qt(0.975, df=n-1) * sd_of_estimates/sqrt(n)
ci_mean <- c(mean_of_estimates - error_margin, mean_of_estimates + error_margin)
# Bootstrap Confidence Interval for the Median
bootstrap_samples <- 10000
medians <- numeric(bootstrap_samples)
for(i in 1:bootstrap_samples) {
sample_data <- sample(index_estimates, size=n, replace=TRUE)
medians[i] <- median(sample_data)
}
ci_median <- quantile(medians, c(0.025, 0.975))
# Print the results
cat("95% CI for the mean:", ci_mean, "\n")
cat("95% CI for the median:", ci_median, "\n")
ci_median
medians
max(medians)
library("tidyverse") # This will load ggplot2, dplyr, readxl, and other essential packages
rm(list = ls())
# Set Working Directory
setwd("H:/My Drive/BIAS/MAIVE")
# Data Import
general  <- read_excel("H:/My Drive/BIAS/DATA/MAIVE.xlsx")
help(read_excel)
library("tidyverse") # This will load ggplot2, dplyr, readxl, and other essential packages
library("writexl")
rm(list = ls())
# Set Working Directory
setwd("H:/My Drive/BIAS/MAIVE")
# Data Import
general  <- read_excel("H:/My Drive/BIAS/DATA/MAIVE.xlsx")
library(readxl)
rm(list = ls())
# Set Working Directory
setwd("H:/My Drive/BIAS/MAIVE")
# Data Import
general  <- read_excel("H:/My Drive/BIAS/DATA/MAIVE.xlsx")
colnames(general)
table(general[general$n_study_m<20,]$metaID)
table(general[general$nEm<20,]$metaID)
table(general[general$n_E_m<20,]$metaID)
# Renaming and Calculating within a Single Pipe
df <- general %>%
rename_with(~c("bs", "sebs", "Ns", "study_id", "meta_id", "ncoefm"), .cols = c("E2", "SE2", "N2", "studyID", "metaID", "n_E_m")) %>%
group_by(meta_id) %>%
mutate(#onecoefm = sum(n_E_s == 1)/ncoefm
onecoefm = sum(n_E_s == 1)/n_study_m) %>%
ungroup()
# Unique meta_ids
numbers <- df$meta_id %>% unique()
# Function to Process Subsets
# Function to Process Subsets
process_subset <- function(data, condition, suffix) {
condition_expr <- rlang::parse_expr(condition) # Parse the condition into an expression
subset_data <- data %>%
filter(!!condition_expr) %>% # Use the parsed expression
select(bs, sebs, Ns, study_id, meta_id, onecoefm, ncoefm)
source("maivebias.R")
results <- as.data.frame(t(MAIVEresults))
colnames(results) <- results[1, ]
results <- results[-1, ] %>%
mutate(across(everything(), as.numeric))
# Add row names as a new column in the data frame
results_with_row_names <- results %>%
tibble::add_column(Row_Names = row.names(results), .before = 1)
# Save the modified data frame to an Excel file
write_xlsx(results_with_row_names, paste0("MAIVE", suffix, ".xlsx"))
# write.csv(df$your_column_name, "your_column_name.csv", row.names = FALSE)
results
}
# Assuming 'df' is your data frame and 'numbers' is a vector of 'meta_id' values
all <- df %>%
select(bs, sebs, Ns, study_id, meta_id, onecoef, ncoefm)
# Split 'all' into a list of data frames, each one corresponding to a unique 'meta_id'
list_of_meta <- split(all, all$meta_id)
# Optionally, if you want to name the list elements with 'Meta_' prefix
names(list_of_meta) <- paste0("Meta_", names(list_of_meta))
dat <- list_of_meta[[paste0("Meta_", 1)]]
maive(dat=dat, method=method, weight=weight, instrument=instrument, studylevel=studylevel)
source("maivefunction.R")
# OPTIONS:
# method: PET:1, PEESE:2, PET-PEESE:3, EK:4 (default 3)
method <- 3
# weighting: default no weight: 0 ; weights: 1, adjusted weights: 2 (default 0)
weight <- 0
# instrumenting (default 1)
instrument <- 1
# correlation at study level: none: 0 (default), fixed effects: 1, cluster: 2
studylevel <-0
maive(dat=dat, method=method, weight=weight, instrument=instrument, studylevel=studylevel)
a <- maive(dat=dat, method=method, weight=weight, instrument=instrument, studylevel=studylevel)
maive(dat=dat, method=method, weight=weight, instrument=instrument, studylevel=studylevel)
maive(dat=dat, method=method, weight=weight, instrument=instrument, studylevel=studylevel)
maive(dat=dat, method=method, weight=weight, instrument=instrument, studylevel=studylevel)
library("tidyverse") # This will load ggplot2, dplyr, readxl, and other essential packages
library("writexl")
library("readxl")
rm(list = ls())
# Set Working Directory
setwd("H:/My Drive/BIAS/MAIVE")
# Data Import
general  <- read_excel("H:/My Drive/BIAS/DATA/MAIVE.xlsx")
# Renaming and Calculating within a Single Pipe
df <- general %>%
rename_with(~c("bs", "sebs", "Ns", "study_id", "meta_id", "ncoefm"), .cols = c("E2", "SE2", "N2", "studyID", "metaID", "n_E_m")) %>%
group_by(meta_id) %>%
mutate(#onecoefm = sum(n_E_s == 1)/ncoefm
onecoefm = sum(n_E_s == 1)/n_study_m) %>%
ungroup()
# Unique meta_ids
numbers <- df$meta_id %>% unique()
# Function to Process Subsets
# Function to Process Subsets
process_subset <- function(data, condition, suffix) {
condition_expr <- rlang::parse_expr(condition) # Parse the condition into an expression
subset_data <- data %>%
filter(!!condition_expr) %>% # Use the parsed expression
select(bs, sebs, Ns, study_id, meta_id, onecoefm, ncoefm)
source("maivebias.R")
results <- as.data.frame(t(MAIVEresults))
colnames(results) <- results[1, ]
results <- results[-1, ] %>%
mutate(across(everything(), as.numeric))
# Add row names as a new column in the data frame
results_with_row_names <- results %>%
tibble::add_column(Row_Names = row.names(results), .before = 1)
# Save the modified data frame to an Excel file
write_xlsx(results_with_row_names, paste0("MAIVE", suffix, ".xlsx"))
# write.csv(df$your_column_name, "your_column_name.csv", row.names = FALSE)
results
}
# Assuming 'df' is your data frame and 'numbers' is a vector of 'meta_id' values
all <- df %>%
select(bs, sebs, Ns, study_id, meta_id, onecoef, ncoefm)
# Split 'all' into a list of data frames, each one corresponding to a unique 'meta_id'
list_of_meta <- split(all, all$meta_id)
# Optionally, if you want to name the list elements with 'Meta_' prefix
names(list_of_meta) <- paste0("Meta_", names(list_of_meta))
# Applying the Function to Different Subsets
object<-c("E","SE","F","Hausman","CV_of_Chi2", "Obs")
allMAIVE <- process_subset(df, "TRUE", "all")
View(allMAIVE)
object<-c("pE","pSE","pF","pHausman","pCV" , "pObs")
publishedMAIVE <- process_subset(df, "df$studyPublishD == 1", "p")
object<-c("npE","npSE","npF","npHausman","npCV" , "npObs")
neverPublishedMAIVE <- process_subset(df, "np == 1 & !is.na(np)", "np")
object<-c("wpE","wpSE","wpF","wpHausman","wpCV" ,"wpObs")
workingPapersMAIVE <- process_subset(df, "studyPublishD == 0", "wp")
setwd("H:/My Drive/BIAS/bias/MAIVE")
object<-c("wpsE","wpsSE","wpsF","wpsHausman","wpsCV", "wpsObs")
workingPapersSansNpMAIVE <- process_subset(df, "studyPublishD == 0 & np == 0", "wps")
